# 考察・戦略

- 初期化
    - 5000
    - 4250
    - randint(4000, 6000)

- 重みの更新
    - クエリごとに更新
        - 初期値4250のdijkstraで90G弱くらい
    - 移動平均をとる
        - 移動平均の更新材料は、一辺の長さがある程度推定できてからのほうがよさそう
        - 経路長/辺の数 みたいな大雑把な値だと、実際の値と乖離が生じやすい
    - predicted_cost の活用
        - 実績値は経路長でしか取得できないが、経路の推定値は一辺ごとに取得できる。
    - 窓関数の利用
        - 更新するときに、同一方向の隣接辺のコストも使う

- 最短経路を眺める
    - 直線多め
    - 4回くらいは曲がる
    - 最短経路とほぼ同じ辺の総数で到達している

- マップを眺める
    - **短い区間、長い区間同士はそれぞれ連続している**
    - 窓関数を用いて、平均値が高い区間を落とすのが有効か?
    - **マップは2種類あり、ファイル単位でいずれかが生成される**
        - レーン上のすべての辺が同程度の重みをもつマップ
        - レーン上の辺が、あるindexを境に別々の重みをもつマップ
            - どちらのマップかわかれば、モデルを切り替えられる

- もっといろんな経路を試せないか?
    - 最短経路のうち1箇所を通行止めにしてみる
        - strategy3で試す、平均はほぼ変わらない
    - 最短経路のうち2箇所を通行止めにしてみる
        - strategy3で試す、平均はほぼ変わらない

- a/b のペナルティを与えられないか?
    - a/bが1に近い = よい経路
    - 後半は推定値に関係なく、a/bが1に近かった経路を優先して通れるようにする?

- 別の問題として解いてみる
    - 最短経路問題
        - Dijkstra
    - 線形回帰
        - SGDRegressorで[増分学習](https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning)
        - ミニバッチごとにRidge

- 線形回帰
    - Ridge
        - 50~100サンプルくらいで良い感じ
        - solverによっては速い。
        - 連続して赤くなっている箇所は実態をよく反映している
        - 単一のノードが赤くなっている箇所は、実態と異なることが多そう
            - 平滑化が効いた。93G。
        - パラメータチューニングなしではDijkstraと変わらない
            - Ridgeならモデルのパラメータチューニングができる。
    - Lasso
        - 重い
        - 適合しない
    - SGDRegressor による増分学習
        - 精度が低い
    - ElasticNet
        - Ridgeと同じくらい速い
        - RidgeとLassoの間に位置するモデルらしい
        - 84G
        - 極端に強調されたマップになってしまう
    - BayesianRidge
        - 90G
        - サンプル数の増大によって使い物にならなくなる

- 非連続な点を含むデータに対する回帰モデル
    - [回帰不連続デザイン(RDD)](https://en.wikipedia.org/wiki/Regression_discontinuity_design)
    - ある時点を境に適合すべき回帰直線が変わるもの
    - 回帰モデルに補正項を追加すればよい
    - 非連続な点は別の方法で推定する必要がある。
        - 傾き最大の箇所でよさそう

- 予測の改善
    - 存在しない辺を削除し、M*(M-1)*2 個のパラメータに落とす
    - Ridgeのパラメータ
        - alpha=1.0: 正則化の強さ
        - fit_intercept=True: 切片を考慮するか
            - Trueがよさそう。切片の期待値が0。
        - max_iter: 反復回数、一部のソルバーが使用する
        - tol=1e-3: 精度
            - 実行時間との兼ね合いで 1e-3 ~ 1e-5 くらい
            - 上3桁が変わらないので 1e-3 で十分
        - solver: "auto": ソルバー
            - 1000サンプルでまともに動くのは "lsqr" "cholesky" "sparse_cg" のいずれか。
            - 速度を考慮すると "lsqr" 一択か。
    - 窓関数
        - 窓関数の形状
            - bartlett
            - hanning
            - ones
        - 隣接辺の数
    - 予測に使う経路の数
        - すべて使う
            - 1000試行のうち何回ごとにモデルを作るか
        - 交差検証
            - RidgeCV
                - "leave-one-out" 91.7G
                - "cv=4" 
        - アンサンブル
            - パフォーマンスは要確認。
    - マップの端の精度の改善
        - np.pad("edge") したら改善する。
        - numbaが使えなくなる。

- マップパラメータMの推定
    - M=1のマップの予測精度が90G、M=0が94Gくらい。
    - M=1のときは、各レーンとも重みの分散が大きくなっていくはず。
    - どうやって推測する?
        - 重み更新後、レーンごとに重みの分散をとる。これを時系列でプロットしてみる。
    - 重みの分散を使って推測してみる
        - BATCH=20, 50, 100で試した。
            - M=100 のほうが安定している。
                - それ以下ではwarmupが必要。BATCH=20なら k<2 の試行は捨てる。
                - 早く判定したいので、最初はBATCH多めにするか...
            - M=0のときは時刻によらず、300前後で横ばいになる。
            - M=1のときは時間経過に応じて、400前後まで線形に上昇する。
                - 上昇傾向を傾きで見抜くのは難しい。
                - 1/4 ~ 1/2 経過時の標準偏差の平均が300を超えるかを基準にすれば十分では?
    - マップパラメータMがわかったら?
        - M=0 とわかれば、((M-1)*M*2) 個の説明変数からなるRidgeを使えばよさそう。
        - M=1 だと説明変数の追加が必要。
            - 境界indexを何らかの方法で取得する。
            - 説明変数 D = {0, 1} を追加し、cutoffの左側を0、右側を1に設定する
            - 重みと一緒にRidgeに食わせる。
        - 説明変数は最小限にしたほうがよい
            - 説明変数そのままなら93Gはキープできるはず
            - 説明変数を倍にしてRidgeを回すと精度が91G程度まで落ちる
            - 観測値として追加してやれば十分では?
    - 境界indexをどうやって計算する?
        - レーンごとに `p=1...29` を境界として `lane[:p], lane[p:]` に分割
        - 平均の和が最大、もしくは分散の和が最小になるところで切ればよいか。
            - 0050.txtで試したところ、平均の和が最大となるindexを境界indexとみなせばよいことがわかった。
    - 境界indexを使って推測してみる
        - 境界indexが簡単に取得できることが分かった
        - 境界で分割した2群間の有意差を検定すればよくないか?
            -  `lane[:p], lane[p:]` の2群に対して Welch のt検定
            - 60組もあればいい感じに推測できるはず
                - M=0 では平均に差がない。
                - M=1 では平均に差が生じうる。
            - 上手くいかなかった
                - p>0.05 を超えることが稀

- 高速化
    - Numba
    - Numpy配列の使い回し